// AUTO-GENERATED FILE. DO NOT MODIFY.
// This file is auto-generated by the Ballerina OpenAPI tool.

// Copyright (c) 2025, WSO2 LLC. (http://www.wso2.com).
//
// WSO2 LLC. licenses this file to you under the Apache License,
// Version 2.0 (the "License"); you may not use this file except
// in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

import ballerina/constraint;
import ballerina/http;

# Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model.
public type ChatCompletionFunctionCall record {
    # The name of the function to call.
    string name;
    # The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
    string arguments;
};

public type ChatCompletionMessageToolCall record {
    # The ID of the tool call.
    string id;
    toolCallType 'type;
    ChatCompletionMessageToolCall_function 'function;
};

@deprecated
public type ChatCompletionRequestFunctionMessage record {
    # The role of the messages author, in this case `function`.
    "function" role;
    # The contents of the function message.
    string? content;
    # The name of the function to call.
    string name;
};

public type CreateCompletionRequest record {
    # The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.
    # 
    # Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.
    string|string[]? prompt = "<|endoftext|>";
    # Generates `best_of` completions server-side and returns the "best" (the one with the highest log probability per token). Results cannot be streamed.
    # 
    # When used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return â€“ `best_of` must be greater than `n`.
    # 
    # **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
    int? best_of = 1;
    # Echo back the prompt in addition to the completion
    boolean? echo = false;
    # Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
    decimal? frequency_penalty = 0;
    # Modify the likelihood of specified tokens appearing in the completion.
    # 
    # Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](https://platform.openai.com/tokenizer?view=bpe) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
    # 
    # As an example, you can pass `{"50256": -100}` to prevent the <|endoftext|> token from being generated.
    record {|int...;|}? logit_bias?;
    # Include the log probabilities on the `logprobs` most likely output tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.
    # 
    # The maximum value for `logprobs` is 5.
    int? logprobs?;
    # The maximum number of [tokens](https://platform.openai.com/tokenizer) that can be generated in the completion.
    # 
    # The token count of your prompt plus `max_tokens` cannot exceed the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
    int? max_tokens = 16;
    # How many completions to generate for each prompt.
    # 
    # **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
    int? n = 1;
    # Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
    decimal? presence_penalty = 0;
    # If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.
    # 
    # Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
    int? seed?;
    # Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
    string|string[]?? stop?;
    # Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).
    boolean? 'stream = false;
    # The suffix that comes after a completion of inserted text.
    # 
    # This parameter is only supported for `gpt-3.5-turbo-instruct`.
    string? suffix?;
    chatCompletionStreamOptions? stream_options?;
    # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
    # 
    # We generally recommend altering this or `top_p` but not both.
    decimal? temperature = 1;
    # An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    # 
    # We generally recommend altering this or `temperature` but not both.
    decimal? top_p = 1;
    # A unique identifier representing your end-user, which can help to monitor and detect abuse.
    string user?;
};

public type ChatCompletionsRequestCommon record {
    # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
    # We generally recommend altering this or `top_p` but not both.
    decimal? temperature = 1;
    # An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    # We generally recommend altering this or `temperature` but not both.
    decimal? top_p = 1;
    # If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.
    boolean? 'stream = false;
    # Up to 4 sequences where the API will stop generating further tokens.
    string|string[]? stop?;
    # The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens).
    # 
    # This value is now deprecated in favor of `max_completion_tokens`, and is not compatible with o1 series models.
    int? max_tokens = 4096;
    # An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.
    int? max_completion_tokens?;
    # Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
    @constraint:Number {minValue: -2, maxValue: 2}
    decimal? presence_penalty = 0;
    # Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
    @constraint:Number {minValue: -2, maxValue: 2}
    decimal? frequency_penalty = 0;
    # Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
    record {}? logit_bias?;
    # A unique identifier representing your end-user, which can help Azure OpenAI to monitor and detect abuse.
    string user?;
};

# Represents the Queries record for the operation: embeddings_create
public type EmbeddingsCreateQueries record {
    # api version
    string api\-version;
};

# Represents a completion response from the API. Note: both the streamed and non-streamed response objects share the same shape (unlike the chat endpoint).
public type CreateCompletionResponse record {
    # A unique identifier for the completion.
    string id;
    # The list of completion choices the model generated for the input prompt.
    CreateCompletionResponse_choices[] choices;
    # The Unix timestamp (in seconds) of when the completion was created.
    int created;
    # The model used for completion.
    string model;
    PromptFilterResults prompt_filter_results?;
    # This fingerprint represents the backend configuration that the model runs with.
    # 
    # Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
    string system_fingerprint?;
    # The object type, which is always "text_completion"
    "text_completion" 'object;
    completionUsage usage?;
};

public type CreateCompletionResponse_logprobs record {
    int[] text_offset?;
    decimal[] token_logprobs?;
    string[] tokens?;
    record {||}[] top_logprobs?;
};

# Provides a set of configurations for controlling the behaviours when communicating with a remote HTTP endpoint.
@display {label: "Connection Config"}
public type ConnectionConfig record {|
    # Provides Auth configurations needed when communicating with a remote HTTP endpoint.
    http:BearerTokenConfig|ApiKeysConfig auth;
    # The HTTP version understood by the client
    http:HttpVersion httpVersion = http:HTTP_2_0;
    # Configurations related to HTTP/1.x protocol
    ClientHttp1Settings http1Settings?;
    # Configurations related to HTTP/2 protocol
    http:ClientHttp2Settings http2Settings?;
    # The maximum time to wait (in seconds) for a response before closing the connection
    decimal timeout = 60;
    # The choice of setting `forwarded`/`x-forwarded` header
    string forwarded = "disable";
    # Configurations associated with request pooling
    http:PoolConfiguration poolConfig?;
    # HTTP caching related configurations
    http:CacheConfig cache?;
    # Specifies the way of handling compression (`accept-encoding`) header
    http:Compression compression = http:COMPRESSION_AUTO;
    # Configurations associated with the behaviour of the Circuit Breaker
    http:CircuitBreakerConfig circuitBreaker?;
    # Configurations associated with retrying
    http:RetryConfig retryConfig?;
    # Configurations associated with inbound response size limits
    http:ResponseLimitConfigs responseLimits?;
    # SSL/TLS-related options
    http:ClientSecureSocket secureSocket?;
    # Proxy server related options
    http:ProxyConfig proxy?;
    # Enables the inbound payload validation functionality which provided by the constraint package. Enabled by default
    boolean validation = true;
    # Enables relaxed data binding on the client side. When enabled, `nil` values are treated as optional, 
    # and absent fields are handled as `nilable` types. Enabled by default.
    boolean laxDataBinding = true;
|};

# The schema for the response format, described as a JSON Schema object.
public type ResponseFormatJsonSchemaSchema record {
};

public type generateImagesResponse record {
    # The unix timestamp when the operation was created.
    int created;
    # The result data of the operation, if successful
    imageResult[] data;
};

public type contentFilterDetectedWithCitationResult_citation record {
    string URL?;
    string license?;
};

# Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model.
# 
# # Deprecated
@deprecated
public type ChatCompletionStreamResponseDelta_function_call record {
    # The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
    string arguments?;
    # The name of the function to call.
    string name?;
};

# Defines the format of the output.
public type audioResponseFormat "json"|"text"|"srt"|"verbose_json"|"vtt";

# Whether to enable parallel function calling during tool use.
public type ParallelToolCalls boolean;

# Represents the Queries record for the operation: ChatCompletions_Create
public type ChatCompletionsCreateQueries record {
    # api version
    string api\-version;
};

# Represents the Queries record for the operation: Translations_Create
public type TranslationsCreateQueries record {
    # api version
    string api\-version;
};

public type contentFilterResultBase record {
    boolean filtered;
};

public type ResponseFormatJsonSchema_json_schema record {
    # A description of what the response format is for, used by the model to determine how to respond in the format.
    string description?;
    # The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
    string name;
    ResponseFormatJsonSchemaSchema schema?;
    # Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the `schema` field. Only a subset of JSON Schema is supported when `strict` is `true`.
    boolean? strict = false;
};

public type ChatCompletionNamedToolChoice_function record {
    # The name of the function to call.
    string name;
};

public type contentFilterDetectedWithCitationResult record {
    *contentFilterDetectedResult;
    boolean filtered;
    contentFilterDetectedWithCitationResult_citation citation?;
};

# Provides API key configurations needed when communicating with a remote HTTP endpoint.
public type ApiKeysConfig record {|
    string api\-key;
|};

# Content filtering results for zero or more prompts in the request. In a streaming request, results for different prompts may arrive at different times or in different orders.
public type PromptFilterResults promptFilterResult[];

# Translation request.
public type createTranslationRequest record {
    # The audio file to translate.
    record {byte[] fileContent; string fileName;} file;
    # An optional text to guide the model's style or continue a previous audio segment. The prompt should be in English.
    string prompt?;
    audioResponseFormat response_format?;
    # The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.
    decimal temperature = 0;
};

# Translation or transcription response when response_format was verbose_json
public type audioVerboseResponse record {
    *audioResponse;
    # Type of audio task.
    "transcribe"|"translate" task?;
    # Language.
    string language?;
    # Duration.
    decimal duration?;
    audioSegment[] segments?;
};

# The style of the generated images.
public type imageStyle "vivid"|"natural";

# Information about the content filtering results.
public type contentFilterResultsBase record {
    contentFilterSeverityResult sexual?;
    contentFilterSeverityResult violence?;
    contentFilterSeverityResult hate?;
    contentFilterSeverityResult self_harm?;
    contentFilterDetectedResult profanity?;
    errorBase 'error?;
};

public type CreateChatCompletionRequest record {
    *ChatCompletionsRequestCommon;
    # A list of messages comprising the conversation so far. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).
    ChatCompletionRequestMessage[] messages;
    #   The configuration entries for Azure OpenAI chat extensions that use them.
    #   This additional specification is only compatible with Azure OpenAI.
    azureChatExtensionConfiguration[] data_sources?;
    # Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
    decimal? frequency_penalty = 0;
    # Modify the likelihood of specified tokens appearing in the completion.
    # 
    # Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
    record {|int...;|}? logit_bias?;
    # Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.
    boolean? logprobs = false;
    # An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.
    int? top_logprobs?;
    # The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.
    # 
    # The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
    int? max_tokens?;
    # How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
    int? n = 1;
    ParallelToolCalls parallel_tool_calls?;
    # Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
    decimal? presence_penalty = 0;
    # An object specifying the format that the model must output. Compatible with [GPT-4o](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-4-and-gpt-4-turbo-models), [GPT-4o mini](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-4-and-gpt-4-turbo-models), [GPT-4 Turbo](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-4-and-gpt-4-turbo-models) and all [GPT-3.5](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-35) Turbo models newer than `gpt-3.5-turbo-1106`.
    # 
    # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured Outputs which guarantees the model will match your supplied JSON schema.
    # 
    # Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.
    # 
    # **Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.
    ResponseFormatText|ResponseFormatJsonObject|ResponseFormatJsonSchema response_format?;
    # This feature is in Beta.
    # If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.
    # Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
    int? seed?;
    # Up to 4 sequences where the API will stop generating further tokens.
    string|string[]? stop?;
    # If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).
    boolean? 'stream = false;
    chatCompletionStreamOptions? stream_options?;
    # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
    # 
    # We generally recommend altering this or `top_p` but not both.
    decimal? temperature = 1;
    # An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    # 
    # We generally recommend altering this or `temperature` but not both.
    decimal? top_p = 1;
    # A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.
    chatCompletionTool[] tools?;
    chatCompletionToolChoiceOption tool_choice?;
    # Deprecated in favor of `tool_choice`.
    # 
    # Controls which (if any) function is called by the model.
    # `none` means the model will not call a function and instead generates a message.
    # `auto` means the model can pick between generating a message or calling a function.
    # Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.
    # 
    # `none` is the default when no functions are present. `auto` is the default if functions are present.
    # 
    # # Deprecated
    @deprecated
    "none"|"auto"|ChatCompletionFunctionCallOption function_call?;
    # Deprecated in favor of `tools`.
    # 
    # A list of functions the model may generate JSON inputs for.
    # 
    # # Deprecated
    @deprecated
    chatCompletionFunctions[] functions?;
    # A unique identifier representing your end-user, which can help to monitor and detect abuse.
    string user?;
};

public type CreateChatCompletionResponse_choices record {
    # The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
    # `length` if the maximum number of tokens specified in the request was reached,
    # `content_filter` if content was omitted due to a flag from our content filters,
    # `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.
    "stop"|"length"|"tool_calls"|"content_filter"|"function_call" finish_reason;
    # The index of the choice in the list of choices.
    int index;
    ChatCompletionResponseMessage message;
    ContentFilterChoiceResults content_filter_results?;
    CreateChatCompletionResponse_logprobs? logprobs;
};

public type ChatCompletionRequestAssistantMessageContentPart ChatCompletionRequestMessageContentPartText|ChatCompletionRequestMessageContentPartRefusal;

public type ChatCompletionRequestSystemMessage record {
    # The contents of the system message.
    string|ChatCompletionRequestSystemMessageContentPart[] content;
    # The role of the messages author, in this case `system`.
    "system" role;
    # An optional name for the participant. Provides the model information to differentiate between participants of the same role.
    string name?;
};

# The size of the generated images.
public type imageSize "1792x1024"|"1024x1792"|"1024x1024";

#   A representation of configuration data for a single Azure OpenAI chat extension. This will be used by a chat
#   completions request that should use Azure OpenAI chat extensions to augment the response behavior.
#   The use of this configuration is compatible only with Azure OpenAI.
public type azureChatExtensionConfiguration record {
    azureChatExtensionType 'type;
};

# The quality of the image that will be generated.
public type imageQuality "standard"|"hd";

public type ChatCompletionMessageToolCallChunk record {
    int index;
    # The ID of the tool call.
    string id?;
    # The type of the tool. Currently, only `function` is supported.
    "function" 'type?;
    ChatCompletionMessageToolCallChunk_function 'function?;
};

# Information about the content filtering category (hate, sexual, violence, self_harm), if it has been detected, as well as the severity level (very_low, low, medium, high-scale that determines the intensity and risk level of harmful content) and if it has been filtered or not. Information about third party text and profanity, if it has been detected, and if it has been filtered or not. And information about customer block list, if it has been filtered and its id.
public type ContentFilterChoiceResults record {
    *contentFilterResultsBase;
    contentFilterDetectedResult protected_material_text?;
    contentFilterDetectedWithCitationResult protected_material_code?;
};

# Represents the Queries record for the operation: ImageGenerations_Create
public type ImageGenerationsCreateQueries record {
    # api version
    string api\-version;
};

public type errorBase record {
    string code?;
    string message?;
};

#   A representation of the additional context information available when Azure OpenAI chat extensions are involved
#   in the generation of a corresponding chat completions response. This context information is only populated when
#   using an Azure OpenAI request configured to use a matching extension.
public type azureChatExtensionsMessageContext record {
    # The data source retrieval result, used to generate the assistant message in the response.
    citation[] citations?;
    # The detected intent from the chat history, used to pass to the next turn to carry over the context.
    string intent?;
};

# Transcription or translation segment.
public type audioSegment record {
    # Segment identifier.
    int id?;
    # Offset of the segment.
    decimal seek?;
    # Segment start offset.
    decimal 'start?;
    # Segment end offset.
    decimal end?;
    # Segment text.
    string text?;
    # Tokens of the text.
    decimal[] tokens?;
    # Temperature.
    decimal temperature?;
    # Average log probability.
    decimal avg_logprob?;
    # Compression ratio.
    decimal compression_ratio?;
    # Probability of 'no speech'.
    decimal no_speech_prob?;
};

public type inline_response_200_usage record {
    int prompt_tokens;
    int total_tokens;
};

public type ResponseFormatJsonObject record {
    # The type of response format being defined: `json_object`
    "json_object" 'type;
};

# The format in which the generated images are returned.
public type imagesResponseFormat "url"|"b64_json";

# Log probability information for the choice.
public type CreateChatCompletionResponse_logprobs record {
    # A list of message content tokens with log probability information.
    chatCompletionTokenLogprob[]? content;
    # A list of message refusal tokens with log probability information.
    chatCompletionTokenLogprob[]? refusal;
};

public type ChatCompletionRequestMessageContentPartText record {
    # The type of the content part.
    "text" 'type;
    # The text content.
    string text;
};

public type chatCompletionRequestAssistantMessage record {
    # The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified.
    string|ChatCompletionRequestAssistantMessageContentPart[]? content?;
    # The refusal message by the assistant.
    string? refusal?;
    # The role of the messages author, in this case `assistant`.
    "assistant" role;
    # An optional name for the participant. Provides the model information to differentiate between participants of the same role.
    string name?;
    ChatCompletionMessageToolCalls tool_calls?;
    chatCompletionRequestAssistantMessage_function_call? function_call?;
};

# Information about the content filtering results.
public type dalleContentFilterResults record {
    contentFilterSeverityResult sexual?;
    contentFilterSeverityResult violence?;
    contentFilterSeverityResult hate?;
    contentFilterSeverityResult self_harm?;
};

# Provides settings related to HTTP/1.x protocol.
public type ClientHttp1Settings record {|
    # Specifies whether to reuse a connection for multiple requests
    http:KeepAlive keepAlive = http:KEEPALIVE_AUTO;
    # The chunking behaviour of the request
    http:Chunking chunking = http:CHUNKING_AUTO;
    # Proxy server related options
    ProxyConfig proxy?;
|};

public type contentFilterSeverityResult record {
    *contentFilterResultBase;
    "safe"|"low"|"medium"|"high" severity;
};

@deprecated
public type chatCompletionFunctions record {
    # A description of what the function does, used by the model to choose when and how to call the function.
    string description?;
    # The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
    string name;
    FunctionParameters parameters?;
};

# The image url or encoded image if successful, and an error otherwise.
public type imageResult record {
    # The image url.
    string url?;
    # The base64 encoded image
    string b64_json?;
    dalleContentFilterResults content_filter_results?;
    # The prompt that was used to generate the image, if there was any revision to the prompt.
    string revised_prompt?;
    dalleFilterResults prompt_filter_results?;
};

public type ChatCompletionRequestMessageContentPartImage_image_url record {
    # Either a URL of the image or the base64 encoded image data.
    string url;
    # Specifies the detail level of the image. Learn more in the [Vision guide](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/gpt-with-vision?tabs=rest%2Csystem-assigned%2Cresource#detail-parameter-settings-in-image-processing-low-high-auto).
    "auto"|"low"|"high" detail = "auto";
};

public type imageGenerationsRequest record {
    # A text description of the desired image(s). The maximum length is 4000 characters.
    @constraint:String {minLength: 1}
    string prompt;
    # The number of images to generate.
    @constraint:Int {minValue: 1, maxValue: 1}
    int n = 1;
    imageSize size?;
    imagesResponseFormat response_format?;
    # A unique identifier representing your end-user, which can help to monitor and detect abuse.
    string user?;
    imageQuality quality?;
    imageStyle style?;
};

# Represents a streamed chunk of a chat completion response returned by model, based on the provided input.
public type createChatCompletionStreamResponse record {
    # A unique identifier for the chat completion. Each chunk has the same ID.
    string id;
    # A list of chat completion choices. Can contain more than one elements if `n` is greater than 1.
    createChatCompletionStreamResponse_choices[] choices;
    # The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.
    int created;
    # The model to generate the completion.
    string model;
    # This fingerprint represents the backend configuration that the model runs with.
    # Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
    string system_fingerprint?;
    # The object type, which is always `chat.completion.chunk`.
    "chat.completion.chunk" 'object;
};

# Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.
public type ChatCompletionFunctionCallOption record {
    # The name of the function to call.
    string name;
};

# Options for streaming response. Only set this when you set `stream: true`.
public type chatCompletionStreamOptions record {
    # If set, an additional chunk will be streamed before the `data: [DONE]` message. The `usage` field on this chunk shows the token usage statistics for the entire request, and the `choices` field will always be an empty array. All other chunks will also include a `usage` field, but with a null value.
    boolean include_usage?;
};

public type ChatCompletionMessageToolCallChunk_function record {
    # The name of the function to call.
    string name?;
    # The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
    string arguments?;
};

public type deploymentid_embeddings_body record {
    # Input text to get embeddings for, encoded as a string. To get embeddings for multiple inputs in a single request, pass an array of strings. Each input must not exceed 2048 tokens in length.
    # Unless you are embedding code, we suggest replacing newlines (\n) in your input with a single space, as we have observed inferior results when newlines are present.
    string|InputItemsString[]? input;
    # A unique identifier representing your end-user, which can help monitoring and detecting abuse.
    string user?;
    # input type of embedding search to use
    string input_type?;
    # The format to return the embeddings in. Can be either `float` or `base64`. Defaults to `float`.
    string? encoding_format?;
    # The number of dimensions the resulting output embeddings should have. Only supported in `text-embedding-3` and later models.
    int? dimensions?;
};

public type chatCompletionRequestToolMessageContentPart ChatCompletionRequestMessageContentPartText;

public type contentFilterDetectedResult record {
    *contentFilterResultBase;
    boolean detected;
};

# A chat completion delta generated by streamed model responses.
public type chatCompletionStreamResponseDelta record {
    # The contents of the chunk message.
    string? content?;
    ChatCompletionStreamResponseDelta_function_call function_call?;
    ChatCompletionMessageToolCallChunk[] tool_calls?;
    # The role of the author of this message.
    "system"|"user"|"assistant"|"tool" role?;
    # The refusal message generated by the model.
    string? refusal?;
};

# Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model.
# 
# # Deprecated
@deprecated
public type chatCompletionRequestAssistantMessage_function_call record {
    # The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
    string arguments;
    # The name of the function to call.
    string name;
};

public type ChatCompletionRequestSystemMessageContentPart ChatCompletionRequestMessageContentPartText;

# The function that the model called.
public type ChatCompletionMessageToolCall_function record {
    # The name of the function to call.
    string name;
    # The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
    string arguments;
};

# The type of the tool call, in this case `function`.
public type toolCallType "function";

# Information about the content filtering category (hate, sexual, violence, self_harm), if it has been detected, as well as the severity level (very_low, low, medium, high-scale that determines the intensity and risk level of harmful content) and if it has been filtered or not. Information about jailbreak content and profanity, if it has been detected, and if it has been filtered or not. And information about customer block list, if it has been filtered and its id.
public type dalleFilterResults record {
    *dalleContentFilterResults;
    contentFilterDetectedResult profanity?;
    contentFilterDetectedResult jailbreak?;
};

# The tool calls generated by the model, such as function calls.
public type ChatCompletionMessageToolCalls ChatCompletionMessageToolCall[];

public type InputItemsString string;

public type ChatCompletionRequestMessageContentPartRefusal record {
    # The type of the content part.
    "refusal" 'type;
    # The refusal message generated by the model.
    string refusal;
};

public type inline_response_200_data record {
    int index;
    string 'object;
    decimal[] embedding;
};

# Information about the content filtering category (hate, sexual, violence, self_harm), if it has been detected, as well as the severity level (very_low, low, medium, high-scale that determines the intensity and risk level of harmful content) and if it has been filtered or not. Information about jailbreak content and profanity, if it has been detected, and if it has been filtered or not. And information about customer block list, if it has been filtered and its id.
public type contentFilterPromptResults record {
    *contentFilterResultsBase;
    contentFilterDetectedResult jailbreak?;
};

public type chatCompletionTool record {
    # The type of the tool. Currently, only `function` is supported.
    "function" 'type;
    FunctionObject 'function;
};

public type inline_response_200_1 CreateChatCompletionResponse|createChatCompletionStreamResponse;

public type inline_response_200_2 audioResponse|audioVerboseResponse;

public type CreateCompletionResponse_choices record {
    # The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
    # `length` if the maximum number of tokens specified in the request was reached,
    # or `content_filter` if content was omitted due to a flag from our content filters.
    "stop"|"length"|"content_filter" finish_reason;
    int index;
    CreateCompletionResponse_logprobs? logprobs;
    string text;
    ContentFilterChoiceResults content_filter_results?;
};

# The parameters the functions accepts, described as a JSON Schema object. See the guide](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format. 
# 
# Omitting `parameters` defines a function with an empty parameter list.
public type FunctionParameters record {
};

public type createChatCompletionStreamResponse_choices record {
    chatCompletionStreamResponseDelta delta;
    CreateChatCompletionResponse_logprobs? logprobs?;
    # The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
    # `length` if the maximum number of tokens specified in the request was reached,
    # `content_filter` if content was omitted due to a flag from our content filters,
    # `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.
    "stop"|"length"|"tool_calls"|"content_filter"|"function_call"? finish_reason;
    # The index of the choice in the list of choices.
    int index;
};

# Translation or transcription response when response_format was json
public type audioResponse record {
    # Translated or transcribed text.
    string text;
};

# Content filtering results for a single prompt in the request.
public type promptFilterResult record {
    int prompt_index?;
    contentFilterPromptResults content_filter_results?;
};

public type FunctionObject record {
    # A description of what the function does, used by the model to choose when and how to call the function.
    string description?;
    # The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
    string name;
    FunctionParameters parameters?;
    # Whether to enable strict schema adherence when generating the function call. If set to true, the model will follow the exact schema defined in the `parameters` field. Only a subset of JSON Schema is supported when `strict` is `true`. Learn more about Structured Outputs in the [function calling guide](docs/guides/function-calling).
    boolean? strict = false;
};

# Transcription request.
public type createTranscriptionRequest record {
    # The audio file object to transcribe.
    record {byte[] fileContent; string fileName;} file;
    # An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.
    string prompt?;
    audioResponseFormat response_format?;
    # The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.
    decimal temperature = 0;
    # The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency.
    string language?;
};

public type ChatCompletionRequestUserMessage record {
    # The contents of the user message.
    string|ChatCompletionRequestUserMessageContentPart[] content;
    # The role of the messages author, in this case `user`.
    "user" role;
    # An optional name for the participant. Provides the model information to differentiate between participants of the same role.
    string name?;
};

public type ChatCompletionRequestMessage ChatCompletionRequestSystemMessage|ChatCompletionRequestUserMessage|chatCompletionRequestAssistantMessage|chatCompletionRequestToolMessage|ChatCompletionRequestFunctionMessage;

public type ChatCompletionRequestMessageContentPartImage record {
    # The type of the content part.
    "image_url" 'type;
    ChatCompletionRequestMessageContentPartImage_image_url image_url;
};

#   A representation of configuration data for a single Azure OpenAI chat extension. This will be used by a chat
#   completions request that should use Azure OpenAI chat extensions to augment the response behavior.
#   The use of this configuration is compatible only with Azure OpenAI.
public type azureChatExtensionType "azure_search"|"azure_cosmos_db";

# Controls which (if any) tool is called by the model.
# `none` means the model will not call any tool and instead generates a message.
# `auto` means the model can pick between generating a message or calling one or more tools.
# `required` means the model must call one or more tools.
# Specifying a particular tool via `{"type": "function", "function": {"name": "my_function"}}` forces the model to call that tool.
# `none` is the default when no tools are present. `auto` is the default if tools are present.
public type chatCompletionToolChoiceOption "none"|"auto"|"required"|chatCompletionNamedToolChoice;

public type ResponseFormatJsonSchema record {
    # The type of response format being defined: `json_schema`
    "json_schema" 'type;
    ResponseFormatJsonSchema_json_schema json_schema;
};

# Represents the Queries record for the operation: Completions_Create
public type CompletionsCreateQueries record {
    # api version
    string api\-version;
};

public type inline_response_200 record {
    string 'object;
    string model;
    inline_response_200_data[] data;
    inline_response_200_usage usage;
};

# Usage statistics for the completion request.
public type completionUsage record {
    # Number of tokens in the prompt.
    int prompt_tokens;
    # Number of tokens in the generated completion.
    int completion_tokens;
    # Total number of tokens used in the request (prompt + completion).
    int total_tokens;
    completionUsage_completion_tokens_details completion_tokens_details?;
};

public type ChatCompletionRequestUserMessageContentPart ChatCompletionRequestMessageContentPartText|ChatCompletionRequestMessageContentPartImage;

# The role of the author of the response message.
public type ChatCompletionResponseMessageRole "assistant";

public type ResponseFormatText record {
    # The type of response format being defined: `text`
    "text" 'type;
};

# citation information for a chat completions response message.
public type citation record {
    # The content of the citation.
    string content;
    # The title of the citation.
    string title?;
    # The URL of the citation.
    string url?;
    # The file path of the citation.
    string filepath?;
    # The chunk ID of the citation.
    string chunk_id?;
};

# Specifies a tool the model should use. Use to force the model to call a specific function.
public type chatCompletionNamedToolChoice record {
    # The type of the tool. Currently, only `function` is supported.
    "function" 'type;
    ChatCompletionNamedToolChoice_function 'function;
};

# Represents a chat completion response returned by model, based on the provided input.
public type CreateChatCompletionResponse record {
    # A unique identifier for the chat completion.
    string id;
    PromptFilterResults prompt_filter_results?;
    # A list of chat completion choices. Can be more than one if `n` is greater than 1.
    CreateChatCompletionResponse_choices[] choices;
    # The Unix timestamp (in seconds) of when the chat completion was created.
    int created;
    # The model used for the chat completion.
    string model;
    # This fingerprint represents the backend configuration that the model runs with.
    # 
    # Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
    string system_fingerprint?;
    # The object type, which is always `chat.completion`.
    "chat.completion" 'object;
    completionUsage usage?;
};

# Represents the Queries record for the operation: Transcriptions_Create
public type TranscriptionsCreateQueries record {
    # api version
    string api\-version;
};

# Proxy server configurations to be used with the HTTP client endpoint.
public type ProxyConfig record {|
    # Host name of the proxy server
    string host = "";
    # Proxy server port
    int port = 0;
    # Proxy server username
    string userName = "";
    # Proxy server password
    @display {label: "", kind: "password"}
    string password = "";
|};

# A chat completion message generated by the model.
public type ChatCompletionResponseMessage record {
    ChatCompletionResponseMessageRole role;
    # The refusal message generated by the model.
    string? refusal;
    # The contents of the message.
    string? content;
    # The tool calls generated by the model, such as function calls.
    ChatCompletionMessageToolCall[] tool_calls?;
    ChatCompletionFunctionCall function_call?;
    azureChatExtensionsMessageContext context?;
};

public type chatCompletionTokenLogprob_top_logprobs record {
    # The token.
    string token;
    # The log probability of this token.
    decimal logprob;
    # A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token.
    int[]? bytes;
};

public type chatCompletionRequestToolMessage record {
    # The role of the messages author, in this case `tool`.
    "tool" role;
    # The contents of the tool message.
    string|chatCompletionRequestToolMessageContentPart[] content;
    # Tool call that this message is responding to.
    string tool_call_id;
};

# Breakdown of tokens used in a completion.
public type completionUsage_completion_tokens_details record {
    # Tokens generated by the model for reasoning.
    int reasoning_tokens?;
};

public type chatCompletionTokenLogprob record {
    # The token.
    string token;
    # The log probability of this token.
    decimal logprob;
    # A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token.
    int[]? bytes;
    # List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested `top_logprobs` returned.
    chatCompletionTokenLogprob_top_logprobs[] top_logprobs;
};
